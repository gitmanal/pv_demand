{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a41ba0-602d-4333-86f7-2aec73da2622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# MLflow setup\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "mlflow.set_experiment('Transformer_TimeSeries_Example')\n",
    "\n",
    "# Load data\n",
    "df = pd.read_parquet('../data/sequences.parquet')\n",
    "df['TIME'] = pd.to_datetime(df['TIME'])\n",
    "df.set_index('TIME', inplace=True)\n",
    "\n",
    "# Parameters\n",
    "SEQUENCE_ID = 1  # Example with one sequence\n",
    "TRAIN_SIZE = 100  # Adjusted to fit the dataset\n",
    "VAL_SIZE = 28     # Adjusted to fit the dataset\n",
    "TEST_SIZE = 28    # Adjusted to fit the dataset\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Cell 2: Define Transformer Model\n",
    "class TransformerTimeSeries(nn.Module):\n",
    "    def __init__(self, input_dim, model_dim, num_heads, num_layers, output_dim, dropout=0.1):\n",
    "        super(TransformerTimeSeries, self).__init__()\n",
    "        self.model_dim = model_dim\n",
    "        \n",
    "        # Embedding layer for input features\n",
    "        self.embedding = nn.Linear(input_dim, model_dim)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, TRAIN_SIZE, model_dim))\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=model_dim, nhead=num_heads, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(model_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Add positional encoding\n",
    "        x = self.embedding(x) + self.positional_encoding[:, :x.size(1), :]\n",
    "        \n",
    "        # Pass through transformer encoder\n",
    "        x = self.transformer_encoder(x)\n",
    "        \n",
    "        # Take the output of the last time step\n",
    "        x = self.fc(x[:, -1, :])\n",
    "        return x\n",
    "\n",
    "# Cell 3: Preprocessing function\n",
    "def preprocess_data(data):\n",
    "    \"\"\"Preprocess sequence data\"\"\"\n",
    "    # Remove outliers\n",
    "    Q1 = data['Power'].quantile(0.25)\n",
    "    Q3 = data['Power'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    data = data[\n",
    "        (data['Power'] >= Q1 - 1.5 * IQR) &\n",
    "        (data['Power'] <= Q3 + 1.5 * IQR)\n",
    "    ].copy()\n",
    "    \n",
    "    # Scale data\n",
    "    scaler = StandardScaler()\n",
    "    data['Power_scaled'] = scaler.fit_transform(data[['Power']])\n",
    "    \n",
    "    return data, scaler\n",
    "\n",
    "# Get sequence data\n",
    "sequence_data = df[df['sequence'] == SEQUENCE_ID].copy()\n",
    "sequence_data, scaler = preprocess_data(sequence_data)\n",
    "\n",
    "# Use only half of the dataset\n",
    "sequence_data = sequence_data.iloc[:len(sequence_data)//2]\n",
    "\n",
    "# Split data\n",
    "train_data = sequence_data['Power_scaled'][:TRAIN_SIZE].values\n",
    "val_data = sequence_data['Power_scaled'][TRAIN_SIZE:TRAIN_SIZE+VAL_SIZE].values\n",
    "test_data = sequence_data['Power_scaled'][TRAIN_SIZE+VAL_SIZE:TRAIN_SIZE+VAL_SIZE+TEST_SIZE].values\n",
    "\n",
    "# Prepare sequences for training\n",
    "def create_sequences(data, seq_length):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        x = data[i:i+seq_length]\n",
    "        y = data[i+seq_length]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "seq_length = 24  # Use 24 time steps as input\n",
    "X_train, y_train = create_sequences(train_data, seq_length)\n",
    "X_val, y_val = create_sequences(val_data, seq_length)\n",
    "X_test, y_test = create_sequences(test_data, seq_length)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Cell 4: Training and evaluation with Transformer\n",
    "model = TransformerTimeSeries(\n",
    "    input_dim=1,  # Input dimension (univariate time series)\n",
    "    model_dim=64,  # Model dimension\n",
    "    num_heads=4,  # Number of attention heads\n",
    "    num_layers=2,  # Number of transformer layers\n",
    "    output_dim=1  # Output dimension (univariate time series)\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Start MLflow run\n",
    "with mlflow.start_run(run_name=f\"sequence_{SEQUENCE_ID}_transformer\"):\n",
    "    try:\n",
    "        # Log parameters\n",
    "        mlflow.log_params({\n",
    "            'sequence': SEQUENCE_ID,\n",
    "            'model_name': 'transformer',\n",
    "            'train_size': TRAIN_SIZE,\n",
    "            'val_size': VAL_SIZE,\n",
    "            'test_size': TEST_SIZE,\n",
    "            'batch_size': BATCH_SIZE,\n",
    "            'epochs': EPOCHS,\n",
    "            'learning_rate': LEARNING_RATE,\n",
    "            'seq_length': seq_length,\n",
    "            'model_dim': 64,\n",
    "            'num_heads': 4,\n",
    "            'num_layers': 2\n",
    "        })\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(EPOCHS):\n",
    "            model.train()\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                y_pred = model(X_batch.unsqueeze(-1))\n",
    "                loss = criterion(y_pred, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_pred = model(X_val.unsqueeze(-1))\n",
    "                val_loss = criterion(val_pred, y_val)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {loss.item()}, Val Loss: {val_loss.item()}\")\n",
    "        \n",
    "        # Test predictions\n",
    "        with torch.no_grad():\n",
    "            test_pred = model(X_test.unsqueeze(-1))\n",
    "            test_pred = scaler.inverse_transform(test_pred.numpy())\n",
    "            test_data_actual = scaler.inverse_transform(y_test.numpy().reshape(-1, 1))\n",
    "        \n",
    "        # Calculate metrics\n",
    "        test_rmse = np.sqrt(mean_squared_error(test_data_actual, test_pred))\n",
    "        test_mae = mean_absolute_error(test_data_actual, test_pred)\n",
    "        \n",
    "        # Log metrics to MLflow\n",
    "        mlflow.log_metrics({\n",
    "            'test_rmse': test_rmse,\n",
    "            'test_mae': test_mae\n",
    "        })\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\nResults for Transformer:\")\n",
    "        print(f\"Test RMSE: {test_rmse:.4f}\")\n",
    "        print(f\"Test MAE: {test_mae:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
